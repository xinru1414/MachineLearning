\section*{Problem 1: Support Vector Machine [20 pts]}
In this question, we are considering the kernel version of SVMs:
$$ \min_{\omega \in \mathbb{R}^d, b, \xi_i \in \mathbb{R}, i=1,2,...,n} \frac{1}{2} ||\omega||^2_2 + C \sum_{i=1}^n \xi_i $$
 $$ s.t. \quad y^{(i)}(\omega^T \phi(x)^{(i)}+b) \geq  1- \xi_i, \forall i=1,...,n $$
$$\xi_i \geq 0, \forall i=1,...,n$$
where $x^{(i)} \in \mathbb{R}^p, i=1,2,...,n$ is the original training data coming along with the label $y^{(i)} \in \left\{-1,1\right\}, C>0$, is a constant and $\phi: \mathbb{R}^p \leftarrow \mathbb{R}^d$ is a mapping function that maps the original data to a new space. Generally speaking, $d>p$ (In fact, $d$ can be $+\infty$). Please answer the following questions. Note that the questions with complexity could be answered with big-O notation.
\subsection*{1.1 Lagrangian form and dual problem [8 pts]}
Given the above functions, the Lagrangian form of SVM is
$$ L(\omega,b, \xi, \alpha, \eta)=\frac{1}{2}||\omega||^2_2 + C \sum_{i=1}^n \xi_i- \sum_{i=1}^n \alpha_i[y^{(i)}(\omega^T\phi(x^{(i)})+b)+\xi_i-1]- \sum_{i=1}^n\eta_i \xi_i $$
where $\alpha, \eta \in \mathbb{R}^n$ are the dual variables.
Now the max-min form of the dual problem is
$$\max_{\alpha \geq 0, \eta \geq 0} \min_{\omega, b, \xi} L(\omega, b, \xi, \alpha, \eta)$$
Please derive the simplified expression of the dual problem, in terms of just $\alpha$, step by step. \\

\begin{soln}
        % Type solution here
\end{soln}

\pagebreak 

\subsection*{1.2 Primal solution and Kernel SVM [12 pts]}
Suppose we have obtained the solution of the dual problem, denoted as $\alpha_i^\star$ for $i = 1,2,...,n$. Following the course slides, we know that the primal solution is
$ \omega^\star = \sum_{i=1}^n \alpha_i y^{(i)} \phi (x^{(i)})$, $b^\star = y^{(k)}-\sum_{i=1}^n \alpha_i y^{(i)} \langle \phi(x^{(i)}), \phi(x^{(k)}) \rangle$,  for any $k$ that $0 < \alpha_k < C$. Write down the corresponding primal solution $\omega^\star$ and $b^\star$. In the sub-questions below, suppose we now have to use the kernel SVM to make a classification decision at some test data point $z \in \mathbb{R}^p$. (12 points, 4 for each subproblem)
\begin{enumerate}
\item {What is the time complexity of making the classification decision if the mapping is given by
{\small $$ \phi(x) = ( \underbrace{p^2, x_{p-1}^2, \cdots, x_1^2}_{\text{p}}, \underbrace{\sqrt[]{2}x_p x_{p-1},\cdots, \sqrt[]{2}x_p x_1}_{\text{p-1}}, \underbrace{\sqrt[]{2}x_{p-1} x_{p-2}, \cdots , \sqrt[]{2}x_{p-1} x_1}_{\text{p-2}}, \cdots, \sqrt[]{2}x_2 x_1, \sqrt[]{2c}x_p,\cdots,\sqrt[]{2c}x_1,c)^T,$$}
with a constant $c > 0$, and we need to compute it from scratch?
}

\begin{soln}
    % Type solution here
\end{soln}

\hfill \linebreak

\item Let $K(u,v) = \phi(u)^T \phi(v)$ where $\phi(\cdot)$ has the same definition as above. Please give a compact form of $K(u,v)$. What is the time complexity of making the classification decision if we directly compute $K(\cdot, \cdot)$?

\begin{soln}
    % Type solution here
\end{soln}

\hfill \linebreak

\item Now please go back to the dual formulation you derived previously. To avoid repetitive computation, one can precompute all the inner products $K(x^{(i)}, x^{(j)}) = \phi(x^{(i)})^T \phi(x^{(j)})$ before solving the dual problem. What is the space complexity of this approach and what might be a problem if $n$ is huge?

\begin{soln}
    % Type solution here
\end{soln}

\end{enumerate}

